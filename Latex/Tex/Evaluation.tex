\section{Evaluation}\label{sec:evaluation}
%We describe the design and implementation of HyperPS. 
%In this section, we evaluate the protection effectiveness and performance of HyperPS by exploiting vulnerabilities and comparing it with traditional KVM through a set of benchmarks.
In this section, we first analyze the security guarantees provided by HyperPS. Then, we evaluate the performance overhead by running a set of benchmarks on both standard KVM and HyperPS.



\subsection{Security Analysis}
 
%In section \ref{sec:design}, we discuss in detail how HyperPS achieves memory isolation, and secure interactions between VM and the hypervisor. Just as the threat model described in the section \ref{threat}, an attacker could subvert the upper VMs by conducting attacks such as cross-domain attack with a malicious virtual machine.
% We first evaluate the security of HyperPS by analyzing how HyperPS can resist various attacks. We organize theses attacks from three perspectives: virtual-machine-related data structures corruption, HyperPS Space violation and DMA attacks.
We first evaluate the security of HyperPS by analyzing how HyperPS can resist various attacks. We organize these attacks from two perspectives: virtual-machine-related data structures corruption, HyperPS Space violation. 

\subsubsection{Virtual-machine-related Data Structure Corruption}%
\label{ssub:virtual_machine_related_data_structure_corruption}
% HyperPS has deprived the HostOS/Hypervisor of privileges of managing physical
HyperPS has deprivileged the HostOS/Hypervisor and interposed all interactions between HostOS and the physical memory.
As illustrated above, VMCS and EPT are the two ultimate structures that adversaries want to tamper with. 
% In our prototype, HyperPS has removed these two data structures from the HostOS/Hypervisor and placed them in the HyperPS Space.
The adversary can maliciously modify the fields in these two structures with regular memory access, if he gained the location of them in HostOS in advance. 
However, in our prototype, HyperPS has removed these two data structures from the HostOS/Hypervisor and placed them in the HyperPS Space.
The adversary can not modify these two data structures with regular memory access. 

As shown in Table \ref{tab3}, we constructed a typical attack (named VMCS Attack) that attempts to modify the field \verb|Guest_CR3| in VMCS with regular memory access. 
As we expected, Because the VMCS has been removed from the Normal Space, this attack failed with information: Segment Fault. 

EPTs are the most critical data structures that manage the VM's physical memory.
As illustrated in Section \ref{ssub:attack_examples}, we focus on resisting the Double-Mapping Attack and Remapping Attack if the attacker gets control over EPTs.
A successful Double-Mapping attack will assign the memory pages that have already been owned by a hostile VM to the victim VM. 
The Remapping attack also manipulates memory mapping relationships. As illustrated in Section \ref{ssub:attack_examples}, 
a successful Remapping attack maliciously modifies the victim VM's EPT Paging-structures to another dedicated page frame. 
In HyperPS Space, because of the Page-Mark Tables and write-protection of EPTs, HyperPS can easily resist these attacks. 
In our prototype, all page table operation functions in the HostOS have been hooked into HyperPS Space. At runtime, all page table modification operations will be checked under the Page-Mark Table. Illegal page table modification or illegal memory share between VMs will be abandoned by HyperPS. 
We implemented a real-world attack to examine if HyperPS can resist these attacks. 
The \verb|CVE-2017-8106| listed in Table \ref{tab3} allows a privileged KVM guest user to access EPT. 
% and conducts attacks via a single-context \verb|INVEPT| instruction.
This attack can also conduct attack by invoking a single-context \verb|INVEPT| instruction with a \verb|NULL| EPTP.
% a \verb|NULL| EPTP
% with a NULL EPT Pointer
HyperPS succeed in detecting the execution of this attack, and HyperPS terminated the malicious VM's execution. 
Because HyperPS has hidden the address of EPT in the HyperPS Space, malicious access to EPT incurred EPT access fault. 
HyperPS has hooked all VMCS operation functions. Any change to EPTP and EPTP List is supervised by HyperPS. Attackers can not change the EPTP with a malicious value.


\iffalse
and terminating the VM's execution 
This attack failed in both  
We implement a real attack, CVE-2017-8106 in kernel version 3.12. A privileged KVM guest OS user accesses EPT, conducts attacks via a single-context INVEPT instruction with a NULL EPT Pointer. Attackers can not implement successfully and incur EPT access fault because HyperPS hides the address of EPT in HyperPS Space and hijacks the loading of EPT. HyperPS verifies the value of EPT to avoid load NULL value. Therefore, HyperPS can avoid subverting memory across VMs including double mapping attack, remapping attack as well as malicious EPT access.
As we expected, 
Because the VMCS has been removed from the Normal Space, this attack failed with information: Segment Fault. 
if a allocated page has been recorded in the Page-Mark Table, HyperPS will check if this mapping change
In HyperPS Space, the Page-Mark Table and write-protection of EPT prevent this kind of attack. For each new mapping to a VM, HyperPS validates whether the page is already in use according to Used field of Page-Mark structure. Meanwhile, the allocated pages must be marked in the Page-Mark table for tracking. Secondly, another challenge is page remapping attack by a compromised hypervisor from a victim VM to a conspiratorial VM. This attack involves remapping a private page to another address space. To defeat this type of attack, HyperPS ensures that whenever a page is released, its content must be zeroed out before creating a new mapping.
We implement a real attack, CVE-2017-8106 in kernel version 3.12. A privileged KVM guest OS user accesses EPT, conducts attacks via a single-context INVEPT instruction with a NULL EPT Pointer. Attackers can not implement successfully and incur EPT access fault because HyperPS hides the address of EPT in HyperPS Space and hijacks the loading of EPT. HyperPS verifies the value of EPT to avoid load NULL value. Therefore, HyperPS can avoid subverting memory across VMs including double mapping attack, remapping attack as well as malicious EPT access.
% attacker is unable to subvert the upper VMs by exploiting the hypervisor vulnerability. 
%But an attack on memory is not limited to loading malicious EPT Pointer.
is another data structure
% The result
% in case
% In our prototype, VMCS and EPT are the two data structures that HyperPS protects in the HyperPS Space.
In this section, we elaborate the security evaluation on how HyperPS achieves memory isolation among VMs through the monitoring interaction data. 
In addition, we analyze the security of HyperPS itself. Table \ref{tab3} shows the real attack instances in line with the above attack model. 
%However, these two attack vectors, regardless of their attack path, both focus on critical interaction data and data on memory, afterwards, modifying more detailed data, such as the VMCS data structure, EPT and EPT Pointer. Thus, we will elaborate on how HyperPS fends of these attack, and Table \ref{tab3} shows the two real attack instances in line with the above attack model. 
% the attack instances listed in Table \ref{tab3} perfectly match the description of attacker, so we will use these two instances to specifically analyze security performance. 


\textbf{Modifying Interaction-Data Attack}
We clarify interaction data including VMCS in Section \ref{interaction}.
To prevent interaction-data leakage, we protect VMCS from the attacker. Firstly, VMCS is hidden in HyperPS Space and can not be accessed by hypervisor. Secondly, functions that can access VMCS are hooked into HyperPS Space, therefore, no functions outside HyperPS Space can access VMCS. The attacker can not get location of VMCS and access it. This prevents attackers from tampering interaction data attacks. We examine protection for VMCS by conducting several attack cases which are widely adopted in real world. Table \ref{tab3} lists all attack cases we used. The attack, named Interaction-data attack, tries to tamper the Guest\_CR3 field in VMCS. This attack fails because it can not access VMCS. 
% The experimental results show that the access failed, malicious interaction-data accessing is prevented successfully.
  According to the above analysis, the attacker can not access VMCS, and can not conduct further attacks. Therefore, the VM interaction data can be protected by HyperPS.
  %cannot be modified. 
\fi

%CVE-2009-2287 allows attacker to provide invalid value of CR3, which is an important data value in VMCS data structure. Because HyperPS would check the value of CR3 before VM entry instruction is conducted, an attacker has no chance to load the value to physical CR3 register successfully. HyperPS protects all important data values in VMCS data structure. In details, interaction between hypervisor and VMs runs in HyperPS Space, VMCS structure used to record context switching data is hidden in HyperPS Space, so an attacker cannot modify VM states during context switching. HyperPS adopts VM-Mark table to ensure that load consistent EPT for every VM, so attacker cannot modify EPTP. Therefore, the VM states cannot be modified. 


\iffalse
\textbf{Subverting Memory Across VMs Attack}
%A kind of attack is subverting memory protection across VMs. 
%Original hypervisor manages the memory of VM through EPT which controls the address translation of VM, so compromised hypervisor can incur malicious memory access attack, such as double mapping attack and remapping attack. However, HyperPS hides the address of EPT in HyperPS Space and hooks all operations about EPT into HyperPS Space. Page tracking technology can make and prevents double mapping and remapping attack. Page tracking technology can ensure that each physical page has only one owner, verify the ownership of each physical page when EPT updates the mapping, and ensure the safe mapping of physical memory. Page tracking  technology can clear the contents of pages when they are completely released, ensuring information security.
%
The main attacks that attackers can execute on subverting memory are double mapping attack and remapping attack.
Firstly, double mapping attack succeeds by allocating memory pages that have already been owned by a hostile VM to a victim VM. Page marking and write-protection of EPT prevent this kind of attack. For each new mapping to a VM, HyperPS validates whether the page is already in use according to Used field of Page-Mark structure. Meanwhile, the allocated pages must be marked in the Page-Mark table for tracking. Secondly, another challenge is page remapping attack by a compromised hypervisor from a victim VM to a conspiratorial VM. This attack involves remapping a private page to another address space. To defeat this type of attack, HyperPS ensures that whenever a page is released, its content must be zeroed out before creating a new mapping.


We implement a real attack, CVE-2017-8106 in kernel version 3.12. A privileged KVM guest OS user accesses EPT, conducts attacks via a single-context INVEPT instruction with a NULL EPT Pointer. Attackers can not implement successfully and incur EPT access fault because HyperPS hides the address of EPT in HyperPS Space and hijacks the loading of EPT. HyperPS verifies the value of EPT to avoid load NULL value. Therefore, HyperPS can avoid subverting memory across VMs including double mapping attack, remapping attack as well as malicious EPT access.
% attacker is unable to subvert the upper VMs by exploiting the hypervisor vulnerability. 
%But an attack on memory is not limited to loading malicious EPT Pointer.
\fi

\begin{table}
\centering
\caption{Hypervisor Attacks Against HyperPS.}\label{tab3}
\begin{tabular}{p{2.8cm}|p{5.5cm}}
\hline
{\itshape\bfseries Attack} & {\itshape\bfseries Description} \\
\hline
VMCS Attack & Load a crafted GUEST\_CR3 value\\
\hline
EPT Attack (CVE-2017-8106) & Load a crafted EPT value \\
\hline
DMA Attack & Access HyperPS Space by DMA \\
\hline
Code Injection Attack & Inject code and cover hooked functions to bypass HyperPS Space \\
\hline
\end{tabular}
\end{table}



% HyperPS Space violation and DMA attacks

\subsubsection{HyperPS Space Violation}%
\label{ssub:hyperps_space_violation}
HyperPS Space is a delicate kernel-level secure and isolated execution space that inherits HostOS's privileges of managing physical memory. As listed in Table \ref{tab3}, the attacker can construct DMA attack or Code Injection Attack to tamper HostOS page tables.  
In this paper, we implement HyperPS Space by using two sets of HostOS page tables and page-table-operation hooking.

The first kind of attack attempts to use regular memory access to maliciously modify the page tables, including the dedicated HostOS kernel page table and the HyperPS Space page table. 
For example, the attack can construct DMA attacks listed in Table \ref{tab3} to overwrite HostOS page tables.  
if HyperPS exposes the HyperPS page table base address, then a compromised kernel can compromise the isolation by changing page table entries. Thus security tools in the HyperPS Space will be tampered with. 
In our prototype, as depicted in Figure \ref{fig:address},
HyperPS removed the corresponding page table entries that are relative to HyperPS page table base address, code and data. 
The compromised HostOS can not find this base address from the entire address space accurately. 
Direct access (e.g., DMA attack) to the original addresses that are allocated to HyperPS in the Normal Space will fault because of Segment Fault.
Besides, HyperPS also set the kernel code non-executable when the processor executes instructions in the HyperPS Space. This can prevent the attacker from exploiting kernel vulnerabilities in the HyperPS Space.

The attacker can also abuse control registers to crash HyperPS Space. The attacker can inject malicious code (as listed in Table \ref{tab3}) to overwrite control registers.  
For example, the register \verb|CR0| controls the W$\oplus${X} privilege of code, the register \verb|CR4| controls if the SMEP mechanism is enabled or not. If an attacker takes over these control registers, he can crash HyperPS Space easily.
In our prototype, HyperPS has hooked all control-register-operation functions into HyperPS Space. Values that are written to control registers are supervised by HyperPS. HyperPS will reject any malicious modification.


% \subsubsection{DMA Attack}%
% \label{ssub:dma_attack}



% can decide SMEP mechanism.


\iffalse
DMA attack is described in detail in section \ref{SG}. Attackers can use this feature to read or corrupt arbitrary memory regions. DMA attack is not a threat to HyperPS, because HyperPS is inherently secure against DMA using IOMMU. Remove the corresponding mapping of the critical data from the page table which IOMMU uses. These critical unmapped data includes the entrance address of HyperPS, data recording Page-Mark structure used in VM isolation, VM-Mark structure and so on. DMA attack that aims at modifying the VM memory or the page tables can also be defeated.
Due to the code of hooked functions including VMCS operations, EPT operations and control register access operations is writable-protection. Accessing CR0 register operation used to set W$\oplus${X} is controlled and page table updating used to change code execution privilege is limited, the attacker can not redirect hooked functions and bypass monitoring.
Some registers access operations including CR0, CR3, CR4, are controlled and hooked to HyperPS Space. CR0 register can control the W$\oplus${X} privilege of code, CR3 can control the loading of the page table and CR4 can decide SMEP mechanism. Protection for page table, hooked functions and regs, plays a role mutually in protection for HyperPS. 

% loading the base address of a malicious HostOS kernel page table, and crash the HyperPS Space.
% if the HyperPS page table base address is exposed to the attacker
Page table protection has been introduced in section \ref{SG}. The entry address mapping of the HyperPS Space page table is deleted from the old page table to prevent the kernel from accessing HyperPS Space directly through the page table mapping. When HyperPS Space is active, the kernel code does not have any executable permissions in case of attacking running processes in HyperPS Space. An attacker may attack in two ways.
First, the attacker may try to directly access the new page table address on the kernel page table by virtual address mapping. When he accesses it, there is page fault due to the absence of address mapping.
Second, the attacker may run kernel code while HyperPS Space is active to attack programs running in HyperPS Space. This can be prevented because of the absence of executable privilege of kernel code.
isolation
In this paper, the security of this space is guaranteed by the synergy of two important techniques: Two Page Table and Page-Table-Operation hooks.
we implement this 
% We have adopt a set of techiques to guarantee the isolation of this execution space.
HyperPS Space is used to hold HyperPS code, data and security tools. Thus, the isolation of HyperPS Space is one of the key requirement of HyperPS. 
In this paper, we achieve 
HyperPS deprive the HostOS of privileges on managing physical memory. 
\fi


\iffalse
\textbf{Destroying HyperPS Space}
HyperPS is created by relying on page tables.
% When HyperPS does not work, the original kernel with high privileges can access any page tables and modify them. It can also access control registers casually, redirect hooked functions used to monitor previously, and even maliciously destroy HyperPS Space through DMA. 
We analyze the protection of HyperPS Space from four aspects, page table modification attack, hooks redirection attack, reg modification attack and DMA attack.


\subsubsection{Page Table Modification Attack}

Page table protection has been introduced in section \ref{SG}. The entry address mapping of the HyperPS Space page table is deleted from the old page table to prevent the kernel from accessing HyperPS Space directly through the page table mapping. When HyperPS Space is active, the kernel code does not have any executable permissions in case of attacking running processes in HyperPS Space. An attacker may attack in two ways.
First, the attacker may try to directly access the new page table address on the kernel page table by virtual address mapping. When he accesses it, there is page fault due to the absence of address mapping.
Second, the attacker may run kernel code while HyperPS Space is active to attack programs running in HyperPS Space. This can be prevented because of the absence of executable privilege of kernel code.



\subsubsection{Hooks Redirection Attack}

Due to the code of hooked functions including VMCS operations, EPT operations and control register access operations is writable-protection. Accessing CR0 register operation used to set W$\oplus${X} is controlled and page table updating used to change code execution privilege is limited, the attacker can not redirect hooked functions and bypass monitoring.

\subsubsection{Reg Modification Attack}

Some registers access operations including CR0, CR3, CR4, are controlled and hooked to HyperPS Space. CR0 register can control the W$\oplus${X} privilege of code, CR3 can control the loading of the page table and CR4 can decide SMEP mechanism. Protection for page table, hooked functions and regs, plays a role mutually in protection for HyperPS. 

\subsubsection{DMA Attack}

%In addition, the memory can be accessed through DMA operations bypassing the MMU, except for accesses by executing memory accessing instructions.
 DMA attack is described in detail in section \ref{SG}. Attackers can use this feature to read or corrupt arbitrary memory regions. DMA attack is not a threat to HyperPS, because HyperPS is inherently secure against DMA using IOMMU. Remove the corresponding mapping of the critical data from the page table which IOMMU uses. These critical unmapped data includes the entrance address of HyperPS, data recording Page-Mark structure used in VM isolation, VM-Mark structure and so on. DMA attack that aims at modifying the VM memory or the page tables can also be defeated.

\fi



\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.95\linewidth]{IMG/performance.pdf}
    \caption{SPECCPU 2006 and Bonnie++ Performance Measurement Results}%
    \label{fig5}
\end{figure}
%修改图为HyperPS

\subsection{Performance Evaluation}
In this section, we evaluate the performance of our HyperPS prototype. All the experiments were conducted on a physical server with a 2.0 GHz 64 core Intel processor and 32 GB memory. The Host system runs Ubuntu 16.04 LTS with a kernel version of 4.4.1. We created more than five virtual machines. The guest is configured with 1GB of memory and runs Ubuntu 16.04 LTS Service with a kernel version of 4.4.1. 

%All experiments are done on a server with 64 cores and 32 GB memory, running at 2.0 GHz and 5 VMs. 

In our prototype, the HostOS kernel is modified so that HyperPS Space is initialized during the boot up sequence. This includes creating a new memory page table for HyperPS, allocating memory pages, as well as creating Page-Mark Table and VM-Mark Table. This process introduces security verification for pages according to Page-Mark Table, and security access to VMCS in HyperPS Space during VM Exit/Entry sequence.
The kernel is modified to place hooks upon some functions.
%control registers, accessing VMCS operations, and accessing EPT operations。
% in order to achieve HyperPS Space, VM Monitoring and VM Isolation.
% The contorl flow jumps to HyperPS Space through the switch gate. 
% introducing worlds switching overhead using switch gate.

%We do not directly compare HyperPS with previous software approaches, as they cannot support all functions simultaneously. 
To evaluate the performance of HyperPS, we experimented with SPECCPU 2006, Bonnie++ and HyperBench\cite{wei2019hyperbench}. The SPECCPU 2006 measure HyperPS's impact to compute-intensive workloads, such as gcc; The Bonnie++ quantifies the performance overhead introduced by HyperPS for file systems; and the HyperBench measure the overall cloud performance under HyperPS, especially impact to the KVM Hypervisor.
All the experiments were repeated fifty times in the cloud environment with HyperPS and in the original cloud environment (the baseline).
In these fifty experiments, we set up different workloads (different numbers of guest VMs with different memory sizes) to simulate different environments in the real cloud environment. 
Besides, we also measured the VM load time to evaluate the impact on VM initialization.
The average results of all the experiments are reported. 


% All the experiments were repeated ten times and the average results are reported.

% In order to assess the effectiveness of all aspects of HyperPS, we conduct a set of experiments to evaluate the performance impact imposed by HyperPS against an original KVM system (the baseline). We run two groups of experiments, compare the performance overhead including benchmarks performance overhead , cloud performance overhead and VM load time.
% For simplicity, we only present the performance evaluation on a server with 64 cores and 32 GB memory, running at 2.0 GHz and guest VM with 2 virtual cores. The version of the hypervisor and guest VM is 3.10.1.
% Different experiments are based on different numbers of guest VMs with different memory size. Both the original hypervisor and HyperPS systems have the same configuration except the protection supported by HyperPS. The deviation of these experiments is insignificant. All the experiments are replicated fifty times and the average results are reported here.



\textbf{Benchmarks Performance}
%All experiments are done on a server with 64 cores and 32 GB memory, running at 2.0 GHz and 5 VMs. 
% In order to obtain the impact of HyperPS on the whole system, we measure HyperPS with microbenchmarks and application benchmarks.
% We use one guest VM with 1 GB memory size.
%All experiments are done 50 times and results are from the average.
We firstly experiment with SPECCPU 2006 to measure the performance overhead introduced by HyperPS.
SPECCPU 2006 consists of several compute-intensive benchmarks, stressing the system's processor and memory subsystems. 
% for compute-intensive applications.
The experiment results are list in Figure \ref{fig5}. As shown in this figure, for the majority of these applications, such as perlbench, bzip2, gcc, gobmk, libquantum and ometpp, 
HyperPS introduced at most 3\% performance overhead. 
However, there are still several benchmarks, such as mcf, astar and xalancbmk whose results are higher than average. 
They caused a performance loss of 6\%. 
Most of these workloads test performance about large and fluctuating memory footprints. 
It's not surprising that HyperPS incurs much more performance loss in these workloads, since the HyperPS takes over physical memory management, and requires context switch and security verification. Frequent memory request incurs HyperPS involvement to index the Page-Mark Table and verifies the legality of EPT Paging-structure updates.
% handles Page-Mark structure and verifies the legality of page mapping when EPT updates.
We can conclude that, for most compute-intensive applications without large memory footprints, HyperPS has a similar performance as the baseline cloud environment. 
For applications with large memory footprints, 
We believe that this performance impact is entirely acceptable for trading this performance loss for higher security.
% These performance impact is still acceptable. Besides, we also achieve

% The high overhead related to memory allocation
% We believe that it is completely acceptable to trade this performance loss for higher security.
% These performance impact is still acceptable
% but they are acceptable

% We can conclude that, for most compute-intensive application without large memory footprints,
% This result
% HyperPS has the similar performance as the baseline cloud environment.
% HyperPS introduced at most 3\% performance overhead.
% most of the SPECCPU 2006 workloads

% To better understand the factor causing the performance overhead, we experiment with compute-bound benchmark (SPEC CPU2006 suite) and one I/O-bound benchmark (Bonnie++) running upon original KVM and HyperPS in a Linux VM. The experiment result described in Figure \ref{fig5}(the last three groups) shows a relatively low cost. Most of the SPEC CPU2006 benchmarks (the first twelve groups) show less than 6\% performance overhead. It's not surprising as there are few OS interactions and these tests are compute-bound. Mcf, astar, and xalancbmk with the highest performance loss allocate lots of memory. HyperPS handles Page-Mark structure and verifies the legality of page mapping when EPT updates. This can incur worlds switching which involves controlling register access and incur VM exit which involves EPT updating.
% and fewer TLB flushes with PCID technique because the two worlds are at the same privileged layer.

We then experiment with Bonnie++ to measure the performance overhead introduced by HyperPS to file systems. 
Bonnie++ sequentially reads/writes data from/to a particular file in different ways. The read/write granularity varies from a character to a block. Furthermore, we also test the time cost of the random access. 
% In our experiments, we specified the file size
% Bonnie++ performed a series of test operations on a file with a known file size.
In our experiments, the target file on which Bonnie++ performs a series of test operations is 1000MB in size. 
% We used Bonnie++ to perform sequential read, write and random access to the file.
Figure \ref{fig5} shows the Bonnie++ measurement results which show the performance overhead on the file system is acceptable too. 
The performance loss of sequential read, write and random access is 2\%, 3\% and 5\%. 
Actually, HyperPS does not interpose much on memory operations for I/O data. 

\iffalse
% The performance loss of
The result
used Bonnie++ 
% We created a file which as 1000MB as the
For Bonnie++, we choose a 1000 MB file to perform the sequential read, write and random access. The performance loss of sequential read, write and random access is 2\%, 3\% and 5\%, 
These performance loss it not high.
the main reason is that HyperPS has no extra memory operations for I/O data. The performance result shows that HyperPS introduces trivial switch overhead of two worlds and trivial overhead of memory isolation of VMs.
\fi

%{\bfseries\textbf\centering{Page-Mark Table}}
\begin{table}
\centering
\caption{execution time of vm operation(s).}\label{tabvm}
%\begin{tabular}{|c{1cm}|c|c|}
\begin{tabular}{p{2cm}|p{1.8cm}|p{2cm}}
\hline
{\itshape\bfseries  Test Case} & {\itshape\bfseries VM Create} & {\itshape\bfseries VM Destroy} \\
\hline
No\_HyperPS & 11.79 s &  1.75 s\\
\hline
With\_HyperPS & 12.97 s & 1.89 s\\ 
\hline
Efficiency & 1.1 & 1.08 \\
\hline
\end{tabular}
\end{table}


HyperBench is a benchmark suite that focuses on measuring virtualization capabilities, especially on measuring HostOS/Hypervisor's capabilities. 
The HyperBench designs 15 hypervisor benchmarks covering CPU, memory, and I/O. The operation in each benchmark triggers hypervisor-level events, which examines the platform's ability in the target area. We used HyperBench to evaluate the performance loss introduced by HyperPS to the HostOS/Hypervisor.
The results are shown in Figure \ref{fig:hyperbench}. For half of the test cases, HyperPS does not introduce performance overhead or little performance loss which is less than 1 percent. 
Inter-Processor Interrupt (IPI) is a special type of interrupt by which one processor may interrupt another in a multiprocessor system. 
HyperPS introduces 4\% performance overhead in this test suit. 
The virtual IPI delivery requires two VM exits and needs to write to VMCS. Because the VMCS has been removed in the Normal Space, access to VMCS requires the involvement of HyperPS.  
HyperPS also poses a higher impact on the cold-access test suit (about 5 percent). 
As mentioned above, the KVM constructs EPT with page faults. Data accessed in the cold-access benchmark is not loaded into physical memory in advance. Thus, the EPT does not map these data before. In our prototype, HyperPS interposes EPT Paging-structure initialization and EPT Page Fault handle procedure. 
All EPT Updates are completed in the HyperPS Space. This is the reason why HyperPS introduced performance loss in this test suit. 
As shown the Figure \ref{fig:hyperbench}, compared with the cold-access benchmark, the performance overhead introduced by HyperPS in the hot-access benchmark is obviously smaller. 
set-pt is another benchmark that operates on EPT. HyperPS introduced about 4 percent more performance overhead than the baseline because all EPT operations are hooked into the HyperPS Space. 

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.9\linewidth]{./IMG/hyperbench.pdf}
    \caption{Virtualization Performance Measurement Results using HyperBench}
    \label{fig:hyperbench}
\end{figure}



\textbf {VM Load Time}
% and World Switch Overhead}
The load time of a VM is a critical aspect of performance to be considered because it influences user experience. We design experiments to evaluate the performance impact of HyperPS for VM loading.
% Experiments are done with 4 VMs, each guest VM is with different memory sizes from 512MB to 4GB.
 %As expected, the VM booting time in HyperPS increases as the memory sizes increase, and the growth amplitude are more and much larger due to the world switch and page tracking caused by frequent memory allocation.
 We measure the impact of completely booting and shutdowning a VM (configured with 2 VCPU and 512MB memory). As Table \ref{tabvm} shown, the booting time is suffered a 1.1 times slowdown under HyperPS, shutdown time is suffered a 1.08 times slowdown, due to the extra overhead of worlds switching and Page-Mark table accessing in HyperPS Space. Such overhead is worth for HyperPS.
